{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import torch\n",
    "from torch.utils.data.dataset import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "class MyLidcDatatset(Dataset):\n",
    "    def __init__(self, IMAGES_PATHS, MASK_PATHS, image_size=512):\n",
    "        \"\"\"\n",
    "        IMAGES_PATHS: list of images paths ['./..data_Image/Image/0001_NI000_slice000.png', \n",
    "        './..data_Image/Image/0001_NI000_slice001.png']\n",
    "\n",
    "        MASKS_PATHS: list of masks paths ['./..data_Image/Mask/0001_MA000_slice000.png',\n",
    "        './..data_Image/Mask/0001_MA000_slice001.png']\n",
    "        \"\"\"\n",
    "        self.image_paths = IMAGES_PATHS\n",
    "        self.mask_paths = MASK_PATHS\n",
    "        self.image_size = image_size\n",
    "        self.transformations = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "    def transform(self, image, mask):\n",
    "        output_image = Image.new(\"RGBA\", (self.image_size, self.image_size))\n",
    "        output_image.paste(image, (0, 0))\n",
    "\n",
    "        mask = mask.convert('L')\n",
    "\n",
    "        image = self.transformations(output_image)\n",
    "        mask = self.transformations(mask)\n",
    "\n",
    "        image, mask = image.type(torch.FloatTensor), mask.type(torch.FloatTensor)\n",
    "\n",
    "        return image, mask\n",
    "\n",
    "    def adjust_dimensions(self, image, mask):\n",
    "        if self.image_size == 512:\n",
    "            return image, mask\n",
    "        \n",
    "        # image resize to the shape\n",
    "        new_resolution = (self.image_size, self.image_size)\n",
    "        resized_image = image.resize(new_resolution, Image.Resampling.LANCZOS)\n",
    "        resized_mask = mask.resize(new_resolution, Image.Resampling.LANCZOS)\n",
    "        \n",
    "        return resized_image, resized_mask\n",
    "        \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        cnt_try = 0\n",
    "        # loop in case if there are any corrupted files\n",
    "        while cnt_try < 10 and index < self.__len__():\n",
    "            try:\n",
    "                image = Image.open(self.image_paths[index])\n",
    "                mask = Image.open(self.mask_paths[index])\n",
    "\n",
    "                image, mask = self.adjust_dimensions(image, mask)\n",
    "\n",
    "                image, mask = self.transform(image, mask)\n",
    "                return image, mask, self.image_paths[index]\n",
    "            \n",
    "            except Exception as e:\n",
    "                # if the image is corrupted, load the next image\n",
    "                print(\"Corrupted file: \",\n",
    "                      self.image_paths[index], '  |  ', sys.exc_info()[0])\n",
    "                print(e)\n",
    "                index += 1\n",
    "                cnt_try += 1\n",
    "        raise (\"Could not resolve Corrupted file: \",\n",
    "               self.image_paths[index], '  |  ', sys.exc_info()[0])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "def load_LIDC(image_size=512, combine_train_val=False, mode='Train'):\n",
    "    # When the mode is Train, it'll load the dataset for training \n",
    "    # In case of mode being Test, it'll load the dataset for testing.\n",
    "    # The combine_train_val indicates if we want to add tran set and validation set.\n",
    "    # image_size will the resize the according to the python PIL image library.\n",
    "\n",
    "    # Directory of Image, Mask folder\n",
    "    IMAGE_DIR = './../data_Image/Image/'\n",
    "    MASK_DIR = './../data_Image/Mask/'\n",
    "    meta = pd.read_csv('./../data_Image/meta.csv')\n",
    "\n",
    "    meta['original_image'] = meta['original_image'].apply(lambda x: (IMAGE_DIR) + x + '.png')\n",
    "    meta['mask_image'] = meta['mask_image'].apply(lambda x: (MASK_DIR) + x + '.png')\n",
    "\n",
    "    train_meta = meta[meta['data_split'] == 'Train']\n",
    "    val_meta = meta[meta['data_split'] == 'Validation']\n",
    "\n",
    "    if mode == 'Test':\n",
    "        test_meta = meta[meta['data_split'] == 'Test']\n",
    "        test_image_paths = list(test_meta['original_image'])\n",
    "        test_mask_paths = list(test_meta['mask_image'])\n",
    "        ds = MyLidcDatatset(test_image_paths, test_mask_paths, image_size)\n",
    "        return ds\n",
    "    \n",
    "    # Get all *npy images into list for Train\n",
    "    train_image_paths = list(train_meta['original_image'])\n",
    "    train_mask_paths = list(train_meta['mask_image'])\n",
    "\n",
    "    # Get all *npy images into list for Validation\n",
    "    val_image_paths = list(val_meta['original_image'])\n",
    "    val_mask_paths = list(val_meta['mask_image'])\n",
    "\n",
    "    if combine_train_val:\n",
    "        train_image_paths.extend(val_image_paths)\n",
    "        train_mask_paths.extend(val_mask_paths)\n",
    "\n",
    "        print(\"*\"*50)\n",
    "        print(\"The lenght of image: {}, mask folders: {} for train\".format(\n",
    "            len(train_image_paths), len(train_mask_paths)))\n",
    "        print(\"*\"*50)\n",
    "\n",
    "        ds = MyLidcDatatset(train_image_paths, train_mask_paths, image_size)\n",
    "        return ds\n",
    "\n",
    "    # not combine train and val\n",
    "    print(\"*\"*50)\n",
    "    print(\"The lenght of image: {}, mask folders: {} for train\".format(\n",
    "        len(train_image_paths), len(train_mask_paths)))\n",
    "    print(\"The lenght of image: {}, mask folders: {} for validation\".format(\n",
    "        len(val_image_paths), len(val_mask_paths)))\n",
    "    print(\"Ratio between Val/ Train is {:2f}\".format(\n",
    "        len(val_image_paths)/len(train_image_paths)))\n",
    "    print(\"*\"*50)\n",
    "\n",
    "    # Create Dataset\n",
    "    train_dataset = MyLidcDatatset(train_image_paths, train_mask_paths, image_size)\n",
    "    val_dataset = MyLidcDatatset(val_image_paths, val_mask_paths, image_size)\n",
    "    # test_dataset = MyLidcDataset(test_image_paths, test_mask_paths)\n",
    "\n",
    "    return train_dataset, val_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 256, 256])\n",
      "torch.Size([1, 1, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "ds = load_LIDC(image_size=256, combine_train_val=True, mode='Test')\n",
    "\n",
    "datal = torch.utils.data.DataLoader(\n",
    "        ds,\n",
    "        batch_size=1,\n",
    "        shuffle=True)\n",
    "\n",
    "data = iter(datal)\n",
    "image, label, image_path = next(data)\n",
    "\n",
    "print(image.shape)\n",
    "print(label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('./../data_Image/Image/0332_NI000_slice002.png',)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "to_pil = transforms.ToPILImage()\n",
    "\n",
    "# Convert the tensor to a PIL Image\n",
    "image = to_pil(label[0])\n",
    "print(image_path)\n",
    "# print(mask_path[0])\n",
    "# Display the image using Matplotlib\n",
    "plt.imshow(image)\n",
    "plt.axis('off')  # Turn off axis labels\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0624_NI003_slice019\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# path = \"./../data_Image/Image/0702_NI004_slice016.png\"\n",
    "filename = os.path.basename(image_path[0]).split(\".\")[0]\n",
    "\n",
    "print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tmp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
